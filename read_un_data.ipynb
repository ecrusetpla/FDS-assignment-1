{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import sklearn as sk\n",
    "import tarfile\n",
    "import io\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tgz_to_dataframe(file_path):\n",
    "    # Open the .tgz file\n",
    "    with tarfile.open(file_path, 'r:gz') as tar:\n",
    "        # Initialize an empty list to store the data\n",
    "        data = []\n",
    "        \n",
    "        # Iterate through each file in the archive\n",
    "        for member in tar.getmembers():\n",
    "            if member.isfile() and member.name.lower().endswith('.txt'):\n",
    "                # Extract the file content\n",
    "                f = tar.extractfile(member)\n",
    "                if f is not None:\n",
    "                    # Read the content and decode it to string, ignoring problematic characters\n",
    "                    content = f.read().decode('utf-8', errors='ignore')\n",
    "                    \n",
    "                    # Process the content\n",
    "                    # For example, split into lines and create a DataFrame\n",
    "                    lines = content.split('\\n')\n",
    "                    df = pd.DataFrame({'text': lines})\n",
    "                    \n",
    "                    # Add filename as a column\n",
    "                    df['filename'] = member.name\n",
    "                    \n",
    "                    # Append the data to our list\n",
    "                    data.append(df)\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    if data:\n",
    "        final_df = pd.concat(data, ignore_index=True)\n",
    "        return final_df\n",
    "    else:\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if no text files were found\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_tgz_to_dataframe(\"./FDS_UNdata/UN General Debate Corpus/UNGDC_1946-2023.tgz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract country ISO, session number, and year using regex\n",
    "df[['country ISO', 'session number', 'year']] = df['filename'].str.extract(r'.*/([A-Z]{3})_(\\d{2})_(\\d{4})')\n",
    "\n",
    "# Convert session number and year to integer types\n",
    "df['session number'] = df['session number'].astype(int)\n",
    "df['year'] = df['year'].astype(int)\n",
    "\n",
    "# Some files from the very first years were not in utf-8 and could not be decoded properly. We drop them (< 2% of total).\n",
    "df = df[df['country ISO'].notna() & df['session number'].notna() & df['year'].notna()].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\EduardCP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\EduardCP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 26>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m score\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Apply preprocessing to the 'text' column\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocess_text)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Create new columns with scores\u001b[39;00m\n\u001b[0;32m     29\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meconomic_score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: calculate_score(x, economic_keywords))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define keywords for each theme\n",
    "economic_keywords = ['economy', 'trade', 'finance', 'budget', 'investment', 'market', 'gdp', 'inflation']\n",
    "social_keywords = ['education', 'healthcare', 'welfare', 'equality', 'poverty', 'human rights', 'social justice']\n",
    "# Add more keyword lists for other themes\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase and tokenize\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove punctuation and stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in string.punctuation and word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Scoring function\n",
    "def calculate_score(tokens, keywords, max_score=10):\n",
    "    keyword_count = sum(1 for word in tokens if word in keywords)\n",
    "    score = min(keyword_count, max_score)  # Cap the score at max_score\n",
    "    return score\n",
    "\n",
    "# Apply preprocessing to the 'text' column\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Create new columns with scores\n",
    "df['economic_score'] = df['processed_text'].apply(lambda x: calculate_score(x, economic_keywords))\n",
    "df['social_issues_score'] = df['processed_text'].apply(lambda x: calculate_score(x, social_keywords))\n",
    "# Add more lines for other themes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
